# Training
def train(num_episodes=250, num_base_stations=3, num_users_per_bs=5):
    env = SmallWirelessNetworkEnv(num_base_stations=num_base_stations, 
                                num_users_per_bs=num_users_per_bs)
    
    # Action space size: 
    # Power adjustments (num_base_stations * n_power_levels) +
    # Bandwidth allocation patterns (num_base_stations * n_bandwidth_levels)
    n_actions = num_base_stations * (env.n_power_levels + env.n_bandwidth_levels)
    
    agent = QLearningAgent(
        n_actions=n_actions,
        alpha=0.1,          # Learning rate
        gamma=0.99,         # Discount factor (same as MADDPG)
        epsilon=1.0,        # Initial exploration rate
        epsilon_decay=0.995,
        epsilon_min=0.01
    )
    
    # Metrics tracking (same structure as MADDPG)
    training_metrics = {
        'rewards': [],
        'snr': [],
        'latency': [],
        'throughput': [],
        'power': [],
        'fairness': [],
        'episode_duration': []
    }
    
    print("Training has started""")
    for episode in range(num_episodes):
        episode_start = time.time()
        state = env.reset()
        episode_rewards = 0
        step_count = 0
        
        while True:
            # Action chosen using the epsilon-greedy strategy
            action = agent.choose_action(state)
            
            #  step in environment
            next_state, reward, done, info = env.step(action)
            
            # Agent learns from experience
            agent.learn(state, action, reward, next_state, done)
            
            # state and accumulated reward has been updated here
            state = next_state
            episode_rewards += reward
            step_count += 1
            
            if done:
                break
        
        #  exploration rate update
        agent.update_epsilon()
        
        episode_end = time.time()
        episode_duration = episode_end - episode_start
        
        # Episode metrics
        episode_metrics = env.get_episode_metrics()
        
        #  training metrics (same as MADDPG)
        training_metrics['rewards'].append(episode_rewards / step_count)
        training_metrics['snr'].append(episode_metrics['snr'])
        training_metrics['latency'].append(episode_metrics['latency'])
        training_metrics['throughput'].append(episode_metrics['throughput'])
        training_metrics['power'].append(episode_metrics['power'])
        training_metrics['fairness'].append(episode_metrics['fairness'])
        training_metrics['episode_duration'].append(episode_duration)
        
        # Print progress every 10 episodes - This is something that also needs heuristic changes for optimal transparency.
        if (episode + 1) % 10 == 0:
            print(f"Episode {episode+1}/{num_episodes}")
            print(f"  Avg Reward: {training_metrics['rewards'][-1]:.2f}")
            print(f"  Avg SNR: {training_metrics['snr'][-1]:.2f} dB")
            print(f"  Avg Latency: {training_metrics['latency'][-1]:.2f} ms")
            print(f"  Avg Throughput: {training_metrics['throughput'][-1]:.2f} Mbps")
            print(f"  Avg Power: {training_metrics['power'][-1]:.2f} W")
            print(f"  Avg Fairness: {training_metrics['fairness'][-1]:.2f}")
            print(f"  Duration: {episode_duration:.2f}s, Epsilon: {agent.epsilon:.3f}")
    
    print("Training complete")
    return agent, env, training_metrics
